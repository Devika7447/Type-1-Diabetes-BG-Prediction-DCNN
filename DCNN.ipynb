{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXdIR71PE2Pv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Path to the folder containing CSV files\n",
        "folder_path = '/content/drive/My Drive/Preprocessed'\n",
        "\n",
        "# Combine all CSV files into one DataFrame\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):  # Ensure it's a CSV file\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        # Specify the semicolon as the delimiter\n",
        "        temp_data = pd.read_csv(file_path, sep=';')\n",
        "        all_data = pd.concat([all_data, temp_data], ignore_index=True)\n",
        "\n",
        "# Display the combined dataset\n",
        "print(\"Combined DataFrame:\")\n",
        "print(all_data.info())\n",
        "print(all_data.head())\n"
      ],
      "metadata": {
        "id": "0SSZRfDpE6zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load one of the CSV files for inspection\n",
        "file_path = '/content/drive/My Drive/Preprocessed/HUPA0015P.csv'  # Replace with the actual path\n",
        "data = pd.read_csv(file_path, sep=';', header=None)  # Use sep=';' to handle semicolon delimiter\n",
        "\n",
        "# Display the first few rows\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "TiDXcpr1E-P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(file_path, sep=';', header=0, engine='python')  # Use 'python' engine for better compatibility\n",
        "print(data.columns)\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "nn3P7JGQE_9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the folder\n",
        "folder_path = '/content/drive/My Drive/Preprocessed'\n",
        "\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Read and split the data\n",
        "        temp_data = pd.read_csv(file_path, header=None)\n",
        "        temp_data_split = temp_data[0].str.split(';', expand=True)\n",
        "        temp_data_split.columns = ['time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
        "\n",
        "        # Combine into the main DataFrame\n",
        "        all_data = pd.concat([all_data, temp_data_split], ignore_index=True)\n",
        "\n",
        "print(all_data.info())\n",
        "print(all_data.head())\n"
      ],
      "metadata": {
        "id": "226SIN-bFBu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Combine all preprocessed CSV files\n",
        "preprocessed_dir = '/content/drive/My Drive/Preprocessed'\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(preprocessed_dir):\n",
        "    file_path = os.path.join(preprocessed_dir, file_name)\n",
        "    # Read and split the data, assuming semicolon delimiter and no header\n",
        "    temp_data = pd.read_csv(file_path, header=None)\n",
        "    temp_data_split = temp_data[0].str.split(';', expand=True)\n",
        "    temp_data_split.columns = ['time', 'glucose', 'calories', 'heart_rate', 'steps', 'basal_rate', 'bolus_volume_delivered', 'carb_input']\n",
        "    all_data = pd.concat([all_data, temp_data_split], ignore_index=True)\n",
        "# Handle missing values\n",
        "all_data = all_data.dropna()\n",
        "\n",
        "# Separate features and target\n",
        "target_column = 'glucose'  # Replace with your actual target column name\n",
        "X = all_data.drop(columns=[target_column, 'time'])  # Drop 'time' column from features\n",
        "y = all_data[target_column]\n",
        "\n",
        "# Convert columns to numeric, errors='coerce' to handle potential non-numeric values\n",
        "X = X.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "pahnnDxjFDw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input, MaxPooling1D\n",
        "\n",
        "# Reshape data for DCNN\n",
        "X_train_dcnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_dcnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Define the DCNN model\n",
        "dcnn_model = Sequential([\n",
        "    Input(shape=(X_train_dcnn.shape[1], 1)),\n",
        "    Conv1D(filters=32, kernel_size=3, dilation_rate=1, activation='relu', padding='same'),  # Added padding='same'\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(filters=64, kernel_size=3, dilation_rate=1, activation='relu', padding='same'),  # Added padding='same'\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "# Compile and train the model\n",
        "dcnn_model.compile(optimizer=tf.keras.optimizers.Adam(clipvalue=1.0), loss='mean_squared_error', metrics=['mae'])\n",
        "history = dcnn_model.fit(X_train_dcnn, y_train, epochs=50, batch_size=32, validation_data=(X_test_dcnn, y_test))\n"
      ],
      "metadata": {
        "id": "-4_1Ig_5FFsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "predictions= dcnn_model.predict(X_test_dcnn)"
      ],
      "metadata": {
        "id": "RhG0Dyv5FILF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of X_test_dcnn:\", X_test_dcnn.shape)"
      ],
      "metadata": {
        "id": "ppxnI3DQFKkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Check for NaN or infinite values in X_test_dcnn\n",
        "print(\"NaN in X_test_dcnn:\", np.isnan(X_test_dcnn).any())\n",
        "print(\"Infinite in X_test_dcnn:\", np.isinf(X_test_dcnn).any())\n",
        "\n",
        "# Check for NaN or infinite values in y_test\n",
        "print(\"NaN in y_test:\", np.isnan(y_test).any())\n",
        "print(\"Infinite in y_test:\", np.isinf(y_test).any())\n"
      ],
      "metadata": {
        "id": "rmYZgRYSFMp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_dcnn = np.nan_to_num(X_test_dcnn)  # Replace NaN with 0\n",
        "y_test = np.nan_to_num(y_test)"
      ],
      "metadata": {
        "id": "-zVPCAMRFS_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Input, MaxPooling1D, Dropout, LSTM, GRU\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "VBRKP08OFUlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Replace NaN in predictions and y_test with 0\n",
        "predictions = np.nan_to_num(predictions)\n",
        "y_test = np.nan_to_num(y_test)\n",
        "\n",
        "# Assuming 'predictions' are your DCNN model's predictions and 'y_test' are the true values\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "print(f\"R-squared (R2): {r2}\")"
      ],
      "metadata": {
        "id": "iHCtpQRzFWSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (Your DCNN model code) ...\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_dcnn = dcnn_model.predict(X_test_dcnn)\n",
        "\n",
        "# Define thresholds for categorization\n",
        "thresholds = [70, 140]  # Example thresholds\n",
        "\n",
        "# Categorize predictions and true values\n",
        "y_test_cat = np.digitize(y_test, thresholds)\n",
        "predictions_dcnn_cat = np.digitize(predictions_dcnn, thresholds)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm_dcnn = confusion_matrix(y_test_cat, predictions_dcnn_cat)\n",
        "\n",
        "# Calculate metrics\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
        "precision_dcnn = precision_score(y_test_cat, predictions_dcnn_cat, average='weighted')\n",
        "recall_dcnn = recall_score(y_test_cat, predictions_dcnn_cat, average='weighted')\n",
        "accuracy_dcnn = accuracy_score(y_test_cat, predictions_dcnn_cat)\n",
        "f1_dcnn = f1_score(y_test_cat, predictions_dcnn_cat, average='weighted')\n",
        "\n",
        "# Print metrics\n",
        "print(\"DCNN Metrics:\")\n",
        "print(\"Confusion Matrix:\", cm_dcnn)\n",
        "print(\"Precision:\", precision_dcnn)\n",
        "print(\"Recall:\", recall_dcnn)\n",
        "print(\"Accuracy:\", accuracy_dcnn)\n",
        "print(\"F1-score:\", f1_dcnn)"
      ],
      "metadata": {
        "id": "zGCbbFHNFb_r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}